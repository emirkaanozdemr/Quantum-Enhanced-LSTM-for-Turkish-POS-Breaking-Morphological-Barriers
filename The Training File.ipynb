{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9964922,"sourceType":"datasetVersion","datasetId":6129880}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A Quantum-Enhanced LSTM Layer","metadata":{"id":"KEXNn0IneNYT"}},{"cell_type":"markdown","source":"One field that so far has been poorly explored in Quantum Machine Learning is Natural Language Processing (NLP), the sub-field of Artificial Intelligence that gives computers the ability to read, write and to some extent comprehend written text.\n\nAs documents are usually presented as sequences of words, historically one of the most successful techniques to manipulate this kind of data has been the Recurrent Neural Network architecture, and in particular a variant called Long Short-Term Memory (LSTM). LSTMs allowed machines to perform translations, classification and intent detection with state-of-the-art accuracy until the advent of Transformer networks. Still, it’s interesting at least from an educational point of view to dig into LSTMs to see what good quantum computing may bring to the field. For a more thorough discussion, please refer to “Quantum Long Short-Term Memory” by Chen, Yoo and Fang (arXiv:2009.01783) and “Recurrent Quantum Neural Networks” by J. Bausch (arXiv:2006.14619).","metadata":{"id":"6s690odPeNYU"}},{"cell_type":"code","source":"!pip install pennylane","metadata":{"id":"y6aVYzNdeNYU","outputId":"e5d39603-bcd8-48eb-ed2c-3bf131574216","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:33:19.604340Z","iopub.execute_input":"2024-11-18T17:33:19.605148Z","iopub.status.idle":"2024-11-18T17:33:27.858181Z","shell.execute_reply.started":"2024-11-18T17:33:19.605103Z","shell.execute_reply":"2024-11-18T17:33:27.857308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom quantum_lstm import QLSTM #./quantum_lstm.py","metadata":{"id":"WeiQu2lLeNYU","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:28.309030Z","iopub.execute_input":"2024-11-18T17:37:28.310103Z","iopub.status.idle":"2024-11-18T17:37:28.314272Z","shell.execute_reply.started":"2024-11-18T17:37:28.310064Z","shell.execute_reply":"2024-11-18T17:37:28.313373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we define the possible tags: determinant, noun, verb.","metadata":{"id":"-LbRPHy2eNYU"}},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/turkish-universal-dependencies/turkish_ud_741.5K.csv') \n# Universal Dependencies Dataset For Turkish - For License Problems the Data Won't Be in GitHub","metadata":{"id":"3pxjiQWPGm2H","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:31.710339Z","iopub.execute_input":"2024-11-18T17:37:31.710669Z","iopub.status.idle":"2024-11-18T17:37:32.209223Z","shell.execute_reply.started":"2024-11-18T17:37:31.710642Z","shell.execute_reply":"2024-11-18T17:37:32.208587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"SSg6gCLqHCZc","outputId":"17f2775c-68e7-42ea-e8cb-4ee541cd0bd3","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:32.533957Z","iopub.execute_input":"2024-11-18T17:37:32.534329Z","iopub.status.idle":"2024-11-18T17:37:32.549963Z","shell.execute_reply.started":"2024-11-18T17:37:32.534296Z","shell.execute_reply":"2024-11-18T17:37:32.549143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequences = df['UPOS Sequence'].str.split(expand=True).stack().unique()\n\nprint(sequences)","metadata":{"id":"87qoAUNiHkgx","outputId":"1a7c53ef-f4f7-46ac-a4e9-574e2c040912","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:33.730090Z","iopub.execute_input":"2024-11-18T17:37:33.730758Z","iopub.status.idle":"2024-11-18T17:37:33.746113Z","shell.execute_reply.started":"2024-11-18T17:37:33.730724Z","shell.execute_reply":"2024-11-18T17:37:33.745209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tag_to_ix = {'_': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'AUX': 4, 'CCONJ': 5, 'DET': 6, 'INTJ': 7, 'NOUN': 8, 'NUM': 9, 'PART': 10, 'PRON': 11, 'PROPN': 12, 'PUNCT': 13, 'SCONJ': 14, 'VERB': 15, 'X': 16} # Assign each tag with a unique index\nix_to_tag = {i:k for k,i in tag_to_ix.items()}","metadata":{"id":"8m5znPlReNYV","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:35.422657Z","iopub.execute_input":"2024-11-18T17:37:35.422969Z","iopub.status.idle":"2024-11-18T17:37:35.427728Z","shell.execute_reply.started":"2024-11-18T17:37:35.422943Z","shell.execute_reply":"2024-11-18T17:37:35.426771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function below tokenizes the sentence and matches the label to each word.","metadata":{"id":"G0g-QL6seNYV"}},{"cell_type":"code","source":"def prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)","metadata":{"id":"V8VYDjSDeNYV","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:37.308822Z","iopub.execute_input":"2024-11-18T17:37:37.309156Z","iopub.status.idle":"2024-11-18T17:37:37.313996Z","shell.execute_reply.started":"2024-11-18T17:37:37.309125Z","shell.execute_reply":"2024-11-18T17:37:37.312941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can prepare the input dataset.","metadata":{"id":"pqQeIES8eNYV"}},{"cell_type":"code","source":"training_data = []\nword_to_ix = {}\ntag_to_ix = {}\n\nfor _, row in df.iterrows():\n    words = row['Sentence'].split()\n    tags = row['UPOS Sequence'].split()\n    training_data.append((words, tags))\n    for word in words:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n    for tag in tags:\n        if tag not in tag_to_ix:\n            tag_to_ix[tag] = len(tag_to_ix)\n\nprint(f\"Training Data: {training_data}\")\nprint(f\"Word to Index: {word_to_ix}\")\nprint(f\"Tag to Index: {tag_to_ix}\")","metadata":{"id":"m3wHtsNJeNYV","outputId":"cb169be8-b3fb-4a08-8e62-2b6b6acf1869","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:40.290151Z","iopub.execute_input":"2024-11-18T17:37:40.290822Z","iopub.status.idle":"2024-11-18T17:37:40.346403Z","shell.execute_reply.started":"2024-11-18T17:37:40.290788Z","shell.execute_reply":"2024-11-18T17:37:40.345303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The idea is to pass the two sequences through the LSTM, which will output the hidden array of vectors [h_0, h_1, h_2, h_3, h_4], one for each word. A dense layer “head” is attached to the LSTM’s outputs to calculate the probability that each word may be a determinant, noun or verb.","metadata":{"id":"74TOq3pWeNYV"}},{"cell_type":"code","source":"class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_qubits=0, num_layers=8):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        if n_qubits > 0:\n            print(\"Tagger will use Quantum LSTM\")\n            self.lstm = QLSTM(embedding_dim, hidden_dim, n_qubits=n_qubits)\n        else:\n            print(\"Tagger will use Classical LSTM\")\n            self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n        tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_logits, dim=1)\n        return tag_scores","metadata":{"id":"aCGfbW3DeNYV","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:52.650319Z","iopub.execute_input":"2024-11-18T17:37:52.650663Z","iopub.status.idle":"2024-11-18T17:37:52.657455Z","shell.execute_reply.started":"2024-11-18T17:37:52.650632Z","shell.execute_reply":"2024-11-18T17:37:52.656541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 8\nhidden_dim = 6\nn_epochs = 300","metadata":{"id":"p6MUdgGpeNYV","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:56.333657Z","iopub.execute_input":"2024-11-18T17:37:56.334317Z","iopub.status.idle":"2024-11-18T17:37:56.337999Z","shell.execute_reply.started":"2024-11-18T17:37:56.334270Z","shell.execute_reply":"2024-11-18T17:37:56.337095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_classical = LSTMTagger(embedding_dim,\n                        hidden_dim,\n                        vocab_size=len(word_to_ix),\n                        tagset_size=len(tag_to_ix),\n                        n_qubits=0)","metadata":{"id":"kY3BcZ5jeNYV","outputId":"5cf8d2c8-3e65-4760-9a32-db8b623ddf69","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:37:58.463328Z","iopub.execute_input":"2024-11-18T17:37:58.463701Z","iopub.status.idle":"2024-11-18T17:37:58.496865Z","shell.execute_reply.started":"2024-11-18T17:37:58.463670Z","shell.execute_reply":"2024-11-18T17:37:58.495980Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{"id":"EQkl4tNueNYV"}},{"cell_type":"markdown","source":"Following the example from the PyTorch website, we train the two networks (classical and quantum LSTM) for 300 epochs.","metadata":{"id":"yxOn1QY2eNYV"}},{"cell_type":"code","source":"def train(model, n_epochs):\n    loss_function = nn.NLLLoss()\n    optimizer = optim.SGD(model.parameters(), lr=1)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n\n    history = {\n        'loss': [],\n        'acc': []\n    }\n    for epoch in range(n_epochs):\n        losses = []\n        preds = []\n        targets = []\n        for sentence, tags in training_data:\n            model.zero_grad()\n            sentence_in = prepare_sequence(sentence, word_to_ix)\n            labels = prepare_sequence(tags, tag_to_ix)\n\n            tag_scores = model(sentence_in)\n            loss = loss_function(tag_scores, labels)\n            loss.backward()\n            optimizer.step()\n            losses.append(float(loss))\n\n            probs = torch.softmax(tag_scores, dim=-1)\n            preds.append(probs.argmax(dim=-1))\n            targets.append(labels)\n\n        avg_loss = np.mean(losses)\n        history['loss'].append(avg_loss)\n\n        preds = torch.cat(preds)\n        targets = torch.cat(targets)\n        corrects = (preds == targets)\n        accuracy = corrects.sum().float() / float(targets.size(0))\n        history['acc'].append(accuracy)\n\n        # Adjust the learning rate\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1} / {n_epochs}: Loss = {avg_loss:.3f}, Acc = {accuracy:.2f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n\n    return history","metadata":{"id":"1rI5jzw3eNYV","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:38:01.100736Z","iopub.execute_input":"2024-11-18T17:38:01.101566Z","iopub.status.idle":"2024-11-18T17:38:01.108782Z","shell.execute_reply.started":"2024-11-18T17:38:01.101532Z","shell.execute_reply":"2024-11-18T17:38:01.107913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_classical = train(model_classical, n_epochs)","metadata":{"collapsed":true,"id":"vCFVuwYheNYV","outputId":"a0d4df5f-8cbe-4cbb-8285-055396cff77c","jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model_classical.state_dict(), \"lstm_tagger_ottoman.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_result(model):\n    with torch.no_grad():\n        input_sentence = training_data[0][0]\n        labels = training_data[0][1]\n        inputs = prepare_sequence(input_sentence, word_to_ix)\n        tag_scores = model(inputs)\n\n        tag_ids = torch.argmax(tag_scores, dim=1).numpy()\n        tag_labels = [ix_to_tag[k] for k in tag_ids]\n        print(f\"Sentence:  {input_sentence}\")\n        print(f\"Labels:    {labels}\")\n        print(f\"Predicted: {tag_labels}\")","metadata":{"id":"GlYsC429eNYW","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:38:05.525902Z","iopub.execute_input":"2024-11-18T17:38:05.526559Z","iopub.status.idle":"2024-11-18T17:38:05.531870Z","shell.execute_reply.started":"2024-11-18T17:38:05.526524Z","shell.execute_reply":"2024-11-18T17:38:05.530817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_result(model_classical)","metadata":{"id":"iz2fLqCteNYW","outputId":"783b821b-be9c-4677-893d-db92185f2d06"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_qubits = 8\n\nmodel_quantum = LSTMTagger(embedding_dim,\n                        hidden_dim,\n                        vocab_size=len(word_to_ix),\n                        tagset_size=len(tag_to_ix),\n                        n_qubits=n_qubits)","metadata":{"id":"D9rQJUmIeNYW","outputId":"9c611e57-75e0-4f0d-979a-f520cabfb54e","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T17:38:13.407362Z","iopub.execute_input":"2024-11-18T17:38:13.407983Z","iopub.status.idle":"2024-11-18T17:38:13.742842Z","shell.execute_reply.started":"2024-11-18T17:38:13.407948Z","shell.execute_reply":"2024-11-18T17:38:13.741671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_quantum = train(model_quantum, n_epochs)","metadata":{"id":"dyuy-a_YeNYW","outputId":"70e880db-a087-413d-a576-032da942efbb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model_quantum.state_dict(), \"qlstm_tagger_ottoman.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_result(model_quantum)","metadata":{"id":"Y1EhSSw1eNYW","outputId":"e5f53597-869a-4db8-f03f-3c47a8db8291"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plot the training history","metadata":{"id":"aj42jq-BeNYW"}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef plot_history(history_classical, history_quantum):\n    loss_c = history_classical['loss']\n    acc_c = history_classical['acc']\n    loss_q = history_quantum['loss']\n    acc_q = history_quantum['acc']\n    n_epochs = max([len(loss_c), len(loss_q)])\n    x_epochs = [i for i in range(n_epochs)]\n\n    fig, ax1 = plt.subplots()\n\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.plot(loss_c, label=\"Classical LSTM loss\", color='orange', linestyle='dashed')\n    ax1.plot(loss_q, label=\"Quantum LSTM loss\", color='red', linestyle='solid')\n\n    ax2 = ax1.twinx()\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.plot(acc_c, label=\"Classical LSTM accuracy\", color='steelblue', linestyle='dashed')\n    ax2.plot(acc_q, label=\"Quantum LSTM accuracy\", color='blue', linestyle='solid')\n\n    plt.title(\"Part-of-Speech Tagger Training\")\n    plt.ylim(0., 1.1)\n    #plt.legend(loc=\"upper right\")\n    fig.legend(loc=\"upper right\", bbox_to_anchor=(1,0.8), bbox_transform=ax1.transAxes)\n\n    plt.savefig(\"pos_training.pdf\")\n    plt.savefig(\"pos_training.png\")\n    plt.show()","metadata":{"id":"Bve1hGvIeNYW"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history(history_classical, history_quantum)","metadata":{"id":"MQa0w-eUeNYW","outputId":"553e3579-b644-4da1-fb63-a09b3451c4cb"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The loss function decreases as a function of the training epoch, and after 300 epochs both networks are able to tag correctly the first sentence. Due to the complexity of the simulation of the quantum circuit, it took approximatively 15 minutes to finish the training, to be compared to a mere 8 seconds for the classical case.","metadata":{"id":"aedohXm2eNYW"}},{"cell_type":"code","source":"","metadata":{"id":"qappggIHeNYW"},"outputs":[],"execution_count":null}]}
